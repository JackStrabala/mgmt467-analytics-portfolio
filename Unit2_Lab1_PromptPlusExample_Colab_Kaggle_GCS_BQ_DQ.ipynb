{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackStrabala/mgmt467-analytics-portfolio/blob/main/Unit2_Lab1_PromptPlusExample_Colab_Kaggle_GCS_BQ_DQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFGDrMH6oASh"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "GFGDrMH6oASh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZFI8W0joASi"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "FZFI8W0joASi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwpFpFQYoASj"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "vwpFpFQYoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYFZvZKhoASj"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "QYFZvZKhoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd75y1hAoASj"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "Fd75y1hAoASj"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Prompt the user to upload their kaggle.json file.\n",
        "# This file contains your Kaggle API credentials.\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create the .kaggle directory if it doesn't exist.\n",
        "# This is where Kaggle expects to find the credentials file.\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Save the uploaded file to the correct location.\n",
        "# Using the first uploaded file as we expect only one (kaggle.json).\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "\n",
        "# Set file permissions to 0600 (owner read/write only).\n",
        "# This is crucial for security to prevent other users from accessing your API key.\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "# Verify the Kaggle installation by printing the version.\n",
        "# This confirms the CLI is installed and can access the credentials.\n",
        "!kaggle --version\n",
        "\n",
        "# Done: Kaggle setup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "KjXQxZVJZ-Bc",
        "outputId": "a54c36ec-14a0-4178-8eda-ff821feb3eca"
      },
      "id": "KjXQxZVJZ-Bc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ba48fed-ab9d-4d47-8d04-008ad9f2fae0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ba48fed-ab9d-4d47-8d04-008ad9f2fae0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Vrae8faFmI"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "Z8Vrae8faFmI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHHGMZsHoASj",
        "outputId": "d17c0c2f-215d-4af4-fa45-54df7d127395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt-467-94721\n",
            "Project: mgmt-467-94721 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt-467-94721\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION # Export the REGION environment variable\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "xHHGMZsHoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEIykdfioASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "BEIykdfioASk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: Check active project and region\n",
        "!gcloud config get-value project\n",
        "import os\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKQVtt4cSqUn",
        "outputId": "33764ad9-c60b-4e30-f2ab-7bf26f39bb4b"
      },
      "id": "gKQVtt4cSqUn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mgmt-467-94721\n",
            "Project: mgmt-467-94721 | Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOb_eekKoASk"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "TOb_eekKoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gf8c7gpoASk"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "3Gf8c7gpoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjRY9WSFoASk"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "MjRY9WSFoASk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "DEnJj-7doASk",
        "outputId": "fc7ced43-9e10-4065-c1f7-ed70453d0cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d83bfe01-652d-4bae-b975-dbe5eefdd8a0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d83bfe01-652d-4bae-b975-dbe5eefdd8a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "\n",
        "!kaggle --version"
      ],
      "id": "DEnJj-7doASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thMOKW2ZoASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "thMOKW2ZoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB2Vy-teoASk"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "pB2Vy-teoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9kJSE4oASl"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "BD9kJSE4oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRhSvGi4oASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "vRhSvGi4oASl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory to store raw data\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using Kaggle CLI to /content/data\n",
        "# The -d flag specifies the dataset, and -p specifies the download path\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "# -o flag overwrites files if they exist\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes in a neat table\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8_U7PESet1j",
        "outputId": "fc91643e-68c6-48ac-c14b-e81bb778fc63"
      },
      "id": "L8_U7PESet1j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 682MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFvFr29hoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "hFvFr29hoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "they2Q5foASl"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "they2Q5foASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5PQ-P1yoASl"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "X5PQ-P1yoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IZooOUoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "b_IZooOUoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3xtQmu5oASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1e1ecc-3574-4d20-ea28-a22d652a176c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating bucket: gs://mgmt467-netflix-b7812b55 in region: us-central1\n",
            "Creating gs://mgmt467-netflix-b7812b55/...\n",
            "\n",
            "Uploading files to gs://mgmt467-netflix-b7812b55/netflix/\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-b7812b55/netflix/movies.csv\n",
            "Copying file:///content/data/raw/README.md to gs://mgmt467-netflix-b7812b55/netflix/README.md\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-b7812b55/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-b7812b55/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-b7812b55/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-b7812b55/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-b7812b55/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 30.9MiB/s\n",
            "\n",
            "Successfully created bucket: mgmt467-netflix-b7812b55 and uploaded files.\n",
            "\n",
            "Benefits of staging data in GCS:\n",
            "- **Consistent Source:** GCS provides a stable and versionable location for your data.\n",
            "- **Scalability:** GCS is highly scalable, handling large datasets easily.\n",
            "- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\n",
            "- **Cost-Effective:** GCS can be a cost-effective storage solution.\n",
            "\n",
            "Verifying contents of gs://mgmt467-netflix-b7812b55/netflix/:\n",
            "gs://mgmt467-netflix-b7812b55/netflix/README.md\n",
            "gs://mgmt467-netflix-b7812b55/netflix/movies.csv\n",
            "gs://mgmt467-netflix-b7812b55/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-b7812b55/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-b7812b55/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-b7812b55/netflix/users.csv\n",
            "gs://mgmt467-netflix-b7812b55/netflix/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "# Create a unique bucket in the specified region\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# Correctly access the REGION environment variable\n",
        "REGION = os.environ.get('REGION')\n",
        "print(f\"Creating bucket: gs://{bucket_name} in region: {REGION}\")\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSVs to the bucket\n",
        "print(f\"\\nUploading files to gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name and explain staging benefits\n",
        "print(f\"\\nSuccessfully created bucket: {bucket_name} and uploaded files.\")\n",
        "print(\"\\nBenefits of staging data in GCS:\")\n",
        "print(\"- **Consistent Source:** GCS provides a stable and versionable location for your data.\")\n",
        "print(\"- **Scalability:** GCS is highly scalable, handling large datasets easily.\")\n",
        "print(\"- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\")\n",
        "print(\"- **Cost-Effective:** GCS can be a cost-effective storage solution.\")\n",
        "\n",
        "# Verify contents (optional but recommended)\n",
        "print(f\"\\nVerifying contents of gs://{bucket_name}/netflix/:\")\n",
        "!gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "O3xtQmu5oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZhif_knoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "kZhif_knoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwgInaDFoASl"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "CwgInaDFoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ApPnyBoASl"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "k7ApPnyBoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mvCSfFhoASl"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "3mvCSfFhoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKjSOx8JoASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa086d09-2285-41c4-bb4c-663e10c34119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'mgmt-467-94721:netflix' successfully created.\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "RKjSOx8JoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bDr5GH1oASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070474d9-5f18-4f1c-e93f-ed1e88c8eb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-b7812b55/netflix/users.csv\n",
            "Waiting on bqjob_r14b1986a573696ae_0000019a12817c39_1 ... (4s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-b7812b55/netflix/movies.csv\n",
            "Waiting on bqjob_r359e11d1a4fe3a92_0000019a1281a1c0_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-b7812b55/netflix/watch_history.csv\n",
            "Waiting on bqjob_r20c27efa2cb0da64_0000019a1281b99d_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-b7812b55/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r16dfee57f38005d3_0000019a1281d5d9_1 ... (2s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-b7812b55/netflix/search_logs.csv\n",
            "Waiting on bqjob_r46e66171e45e4f37_0000019a1281f348_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-b7812b55/netflix/reviews.csv\n",
            "Waiting on bqjob_r2544c8474ef73b66_0000019a12820a5d_1 ... (1s) Current status: DONE   \n",
            "Counting rows in users\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| users      | 10300 |\n",
            "+------------+-------+\n",
            "Counting rows in movies\n",
            "+------------+------+\n",
            "| table_name |  n   |\n",
            "+------------+------+\n",
            "| movies     | 1040 |\n",
            "+------------+------+\n",
            "Counting rows in watch_history\n",
            "+---------------+--------+\n",
            "|  table_name   |   n    |\n",
            "+---------------+--------+\n",
            "| watch_history | 105000 |\n",
            "+---------------+--------+\n",
            "Counting rows in recommendation_logs\n",
            "+---------------------+-------+\n",
            "|     table_name      |   n   |\n",
            "+---------------------+-------+\n",
            "| recommendation_logs | 52000 |\n",
            "+---------------------+-------+\n",
            "Counting rows in search_logs\n",
            "+-------------+-------+\n",
            "| table_name  |   n   |\n",
            "+-------------+-------+\n",
            "| search_logs | 26500 |\n",
            "+-------------+-------+\n",
            "Counting rows in reviews\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| reviews    | 15450 |\n",
            "+------------+-------+\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Load tables (commented)\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "DATASET=\"netflix\" # Make sure dataset is defined\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  # Corrected bq load command\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts\n",
        "for tbl in tables.keys():\n",
        "  # Corrected bq query command using f-string\n",
        "  print(f\"Counting rows in {tbl}\")\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.{tbl}\\`\""
      ],
      "id": "3bDr5GH1oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqEU0_sBoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "yqEU0_sBoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeM4Ik7moASl"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "xeM4Ik7moASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TOxNJk_oASl"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "3TOxNJk_oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nht9dQqOoASl"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "nht9dQqOoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D2oFpiSoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "4D2oFpiSoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08HEz5PooASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f692ed-5be3-4da2-d35a-3b40de779a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((10300, 0.0, 0.0, 11.93), {'n': 0, 'pct_missing_country': 1, 'pct_missing_subscription_plan': 2, 'pct_missing_age': 3})\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Users: % missing per column\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "08HEz5PooASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5abacc8b"
      },
      "source": [
        "### Save & submit checklist:\n",
        "\n",
        "- [ ] Save this notebook to the team Drive.\n",
        "- [ ] Export a `.sql` file with your DQ queries and save to repo.\n",
        "- [ ] Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- [ ] Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts."
      ],
      "id": "5abacc8b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "8c9573e0",
        "outputId": "572b28c5-57c6-4cac-f4e8-a3b1cacdf6e0"
      },
      "source": [
        "# Verification: Single compact summary query for anomaly flags\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  'flag_binge' AS flag_name,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes_capped > 8*60)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.watch_history_robust`\n",
        "UNION ALL\n",
        "SELECT\n",
        "  'flag_age_extreme' AS flag_name,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT\n",
        "  'flag_duration_anomaly' AS flag_name,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "8c9573e0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequest",
          "evalue": "400 Unrecognized name: watch_duration_minutes_capped; Did you mean watch_duration_minutes? at [4:21]; reason: invalidQuery, location: query, message: Unrecognized name: watch_duration_minutes_capped; Did you mean watch_duration_minutes? at [4:21]\n\nLocation: US\nJob ID: 5e4609c2-9096-47c4-8b00-1c5af90391e6\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-948362193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mquery_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Print the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1771\u001b[0m                 \u001b[0;31m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m                 \u001b[0;31m# long-running API, don't delay the next request at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_job_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mis_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1720\u001b[0m                         \u001b[0;31m# `job_retry` predicate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                         \u001b[0mrestart_query_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mjob_failed_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                         \u001b[0;31m# Make sure that the _query_results are cached so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 Unrecognized name: watch_duration_minutes_capped; Did you mean watch_duration_minutes? at [4:21]; reason: invalidQuery, location: query, message: Unrecognized name: watch_duration_minutes_capped; Did you mean watch_duration_minutes? at [4:21]\n\nLocation: US\nJob ID: 5e4609c2-9096-47c4-8b00-1c5af90391e6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4011b829",
        "outputId": "60e47c38-9439-4416-9cd4-9a4b2fa36462"
      },
      "source": [
        "# In movies, compute and summarize flag_duration_anomaly where duration_minutes < 15 or > 480.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 480) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "4011b829",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((24, 22, 2080, 2.21), {'titles_under_15m': 0, 'titles_over_8h': 1, 'total': 2, 'pct_duration_anomaly': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79c4c323",
        "outputId": "839ad588-4990-4438-acfe-52b9754293a4"
      },
      "source": [
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "!bq head --schema=true {project_id}.netflix.movies"
      ],
      "id": "79c4c323",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FATAL Flags parsing error: Unknown command line flag 'schema'\n",
            "Run 'bq.py help' to get help\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20317cb9",
        "outputId": "00412b3f-b371-458d-eec6-35511033121b"
      },
      "source": [
        "# In users, compute and summarize flag_age_extreme if age is <10 or >100.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "20317cb9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((358, 20600, 1.74), {'extreme_age_rows': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b151c53",
        "outputId": "9b7c0745-ed07-4e91-dfe4-57d55f3ceb68"
      },
      "source": [
        "# In watch_history_robust, compute and summarize flag_binge for sessions > 8 hours.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes_capped > 8*60)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "5b151c53",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0, 100000, 0.0), {'sessions_over_8h': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23379d21",
        "outputId": "ae4b05ec-c553-43b1-8da8-b6314d29fb53"
      },
      "source": [
        "# Verification: Show min/median/max before vs after capping\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH before AS (\n",
        "  SELECT\n",
        "    'before' AS which,\n",
        "    MIN(watch_duration_minutes) AS min_val,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_val,\n",
        "    MAX(watch_duration_minutes) AS max_val\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT\n",
        "    'after' AS which,\n",
        "    MIN(watch_duration_minutes_capped) AS min_val,\n",
        "    APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_val,\n",
        "    MAX(watch_duration_minutes_capped) AS max_val\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "23379d21",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('after', 4.4, 51.4, 203.6), {'which': 0, 'min_val': 1, 'median_val': 2, 'max_val': 3})\n",
            "Row(('before', 0.2, 51.1, 799.3), {'which': 0, 'min_val': 1, 'median_val': 2, 'max_val': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "857434d3",
        "outputId": "08e536f0-b914-468f-851e-9fc066f95398"
      },
      "source": [
        "# Create watch_history_robust with watch_duration_minutes_capped at P01/P99; return quantile summaries before/after.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(98)] AS p99\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h, q;\n",
        "\n",
        "-- Quantiles before vs after\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "857434d3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('after', [4.4, 24.6, 41.5, 61.5, 92.0, 203.6]), {'which': 0, 'q': 1})\n",
            "Row(('before', [0.2, 24.8, 41.7, 61.2, 91.7, 799.3]), {'which': 0, 'q': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143550e6",
        "outputId": "02fba0f8-3cba-45af-c9da-34aff837ef2b"
      },
      "source": [
        "# Compute IQR bounds for watch_duration_minutes on watch_history_dedup and report % outliers\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "143550e6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((3552, 100000, 3.55), {'outliers': 0, 'total': 1, 'pct_outliers': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c029865a",
        "outputId": "0b176931-f19d-4240-a098-59c32516ce00"
      },
      "source": [
        "# Verification: Before/after count query comparing raw vs watch_history_dedup\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT 'raw' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'deduplicated' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history_dedup`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "c029865a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('deduplicated', 100000), {'table_name': 0, 'row_count': 1})\n",
            "Row(('raw', 210000), {'table_name': 0, 'row_count': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4541fb9f",
        "outputId": "26ad63c3-ddd7-4947-f35c-0d0813123ef2"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"SELECT h.* FROM `{project_id}.netflix.watch_history` h LIMIT 1\"\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "for row in results:\n",
        "    print(row.keys())"
      ],
      "id": "4541fb9f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['session_id', 'user_id', 'movie_id', 'watch_date', 'device_type', 'watch_duration_minutes', 'progress_percentage', 'action', 'quality', 'location_country', 'is_download', 'user_rating'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3537f77",
        "outputId": "ab0f45e9-ce17-427e-d34c-5d31c2b33565"
      },
      "source": [
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "!bq head --schema=true {project_id}.netflix.watch_history"
      ],
      "id": "b3537f77",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FATAL Flags parsing error: Unknown command line flag 'schema'\n",
            "Run 'bq.py help' to get help\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ca896b6",
        "outputId": "4682f2f9-8e3a-4f33-a4ba-1dbccba282e1"
      },
      "source": [
        "# Detect duplicate groups on (user_id, movie_id, watch_date, device_type) with counts (top 20)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "2ca896b6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('user_03310', 'movie_0640', datetime.date(2024, 9, 8), 'Smart TV', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00391', 'movie_0893', datetime.date(2024, 8, 26), 'Laptop', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06417', 'movie_0590', datetime.date(2024, 1, 15), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_07594', 'movie_0133', datetime.date(2025, 3, 24), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00928', 'movie_0913', datetime.date(2024, 1, 18), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03021', 'movie_0602', datetime.date(2025, 2, 23), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00249', 'movie_0203', datetime.date(2024, 8, 31), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02138', 'movie_0729', datetime.date(2025, 8, 28), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02126', 'movie_0642', datetime.date(2025, 2, 9), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03140', 'movie_0205', datetime.date(2025, 9, 11), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03660', 'movie_0109', datetime.date(2025, 5, 20), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08157', 'movie_0729', datetime.date(2025, 10, 26), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01755', 'movie_0745', datetime.date(2024, 6, 12), 'Mobile', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03043', 'movie_0465', datetime.date(2024, 2, 3), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03176', 'movie_0534', datetime.date(2024, 1, 6), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04657', 'movie_0857', datetime.date(2025, 3, 4), 'Mobile', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_05952', 'movie_0893', datetime.date(2024, 4, 29), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00965', 'movie_0991', datetime.date(2024, 2, 14), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01807', 'movie_0921', datetime.date(2025, 1, 30), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08681', 'movie_0332', datetime.date(2024, 6, 13), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1cb5d3c",
        "outputId": "748f6959-0cdc-4906-d538-9f87a65d4b82"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Verification: Print missingness percentages\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "f1cb5d3c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0.0, 0.0, 11.93), {'pct_missing_country': 0, 'pct_missing_subscription_plan': 1, 'pct_missing_age': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26eddfd",
        "outputId": "885dd029-f2cf-4f32-c04c-d27b271d5d74"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"SELECT * FROM `{project_id}.netflix.users` LIMIT 1\"\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "for row in results:\n",
        "    print(row.keys())"
      ],
      "id": "a26eddfd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['user_id', 'email', 'first_name', 'last_name', 'age', 'gender', 'country', 'state_province', 'city', 'subscription_plan', 'subscription_start_date', 'is_active', 'monthly_spend', 'primary_device', 'household_size', 'created_at'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4655b3e",
        "outputId": "c2c271eb-31d7-407a-c28a-4d865ec0ab21"
      },
      "source": [
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "!bq head --schema=true {project_id}.netflix.users"
      ],
      "id": "f4655b3e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FATAL Flags parsing error: Unknown command line flag 'schema'\n",
            "Run 'bq.py help' to get help\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32519673",
        "outputId": "b17b7562-e0ab-4760-dcce-8a235498bcdc"
      },
      "source": [
        "%load_ext bigquery_magics"
      ],
      "id": "32519673",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bigquery_magics extension is already loaded. To reload it, use:\n",
            "  %reload_ext bigquery_magics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Ft9RmeoASm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5985d88c-807a-452f-fdf0-37177feb2ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('Canada', 6192, 0.0), {'country': 0, 'n': 1, 'pct_missing_subscription_plan': 2})\n",
            "Row(('USA', 14408, 0.0), {'country': 0, 'n': 1, 'pct_missing_subscription_plan': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — MAR by region (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT country,\n",
        "       COUNT(*) AS n,\n",
        "       ROUND(100*COUNTIF(subscription_plan IS NULL)/COUNT(*),2) AS pct_missing_subscription_plan\n",
        "FROM `{project_id}.netflix.users`\n",
        "GROUP BY country\n",
        "ORDER BY pct_missing_subscription_plan DESC;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "_8Ft9RmeoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si9Z_OIEoASm"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "Si9Z_OIEoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6zI2MSmoASm"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why."
      ],
      "id": "n6zI2MSmoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T73XrgnIoASm"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "T73XrgnIoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZegFSkoASm"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "I5ZegFSkoASm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-xK-vcoASm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea15cd1-c181-4369-9325-f75b6a6441c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('user_03310', 'movie_0640', datetime.date(2024, 9, 8), 'Smart TV', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00391', 'movie_0893', datetime.date(2024, 8, 26), 'Laptop', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06417', 'movie_0590', datetime.date(2024, 1, 15), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_07594', 'movie_0133', datetime.date(2025, 3, 24), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00928', 'movie_0913', datetime.date(2024, 1, 18), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03021', 'movie_0602', datetime.date(2025, 2, 23), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00249', 'movie_0203', datetime.date(2024, 8, 31), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02138', 'movie_0729', datetime.date(2025, 8, 28), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02126', 'movie_0642', datetime.date(2025, 2, 9), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03140', 'movie_0205', datetime.date(2025, 9, 11), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03660', 'movie_0109', datetime.date(2025, 5, 20), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08157', 'movie_0729', datetime.date(2025, 10, 26), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01755', 'movie_0745', datetime.date(2024, 6, 12), 'Mobile', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03043', 'movie_0465', datetime.date(2024, 2, 3), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03176', 'movie_0534', datetime.date(2024, 1, 6), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04657', 'movie_0857', datetime.date(2025, 3, 4), 'Mobile', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_05952', 'movie_0893', datetime.date(2024, 4, 29), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00965', 'movie_0991', datetime.date(2024, 2, 14), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01807', 'movie_0921', datetime.date(2025, 1, 30), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08681', 'movie_0332', datetime.date(2024, 6, 13), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "AH-xK-vcoASm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GsXzPdQoASq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "8be490a2-ca76-4e7d-d54e-70f8a4716a9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'mgmt-467-94721'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-592025062.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mproject_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgmt-467-94721'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mgmt-467-94721'"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['mgmt-467-94721']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `{project_id}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"Deduplicated table watch_history_dedup created successfully.\")"
      ],
      "id": "_GsXzPdQoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mrF9O0UoASq"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "6mrF9O0UoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyNoGyV0oASq"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?"
      ],
      "id": "vyNoGyV0oASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFlt_N03oASq"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "YFlt_N03oASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGiM4dQToASq"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "QGiM4dQToASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5zQ5TJmoASq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "21f8aa9a-93bf-4b7e-ccf6-b84488d74ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 Not found: Table mgmt-467-94721:netflix.watch_history_dedup was not found in location US; reason: notFound, message: Not found: Table mgmt-467-94721:netflix.watch_history_dedup was not found in location US\n\nLocation: US\nJob ID: 011d8e5d-3a4e-4d30-85c1-bfc29d2a8424\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1115689281.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mquery_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Print the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1771\u001b[0m                 \u001b[0;31m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m                 \u001b[0;31m# long-running API, don't delay the next request at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_job_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mis_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1720\u001b[0m                         \u001b[0;31m# `job_retry` predicate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                         \u001b[0mrestart_query_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mjob_failed_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                         \u001b[0;31m# Make sure that the _query_results are cached so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 Not found: Table mgmt-467-94721:netflix.watch_history_dedup was not found in location US; reason: notFound, message: Not found: Table mgmt-467-94721:netflix.watch_history_dedup was not found in location US\n\nLocation: US\nJob ID: 011d8e5d-3a4e-4d30-85c1-bfc29d2a8424\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — IQR outlier rate (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "T5zQ5TJmoASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuY6Qv5UoASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6206dfd6-e93a-48d5-e769-e78ab0405865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('after', [4.4, 24.6, 41.5, 61.5, 92.0, 203.6]), {'which': 0, 'q': 1})\n",
            "Row(('before', [0.2, 24.8, 41.8, 61.3, 91.7, 799.3]), {'which': 0, 'q': 1})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(98)] AS p99\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h, q;\n",
        "\n",
        "-- Quantiles before vs after\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "OuY6Qv5UoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYXOx0WioASq"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "iYXOx0WioASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gubOakeIoASq"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why."
      ],
      "id": "gubOakeIoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtXNlTftoASq"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "xtXNlTftoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOJ7CAHkoASq"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "VOJ7CAHkoASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfvn5Vn7oASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dd1585-c0b6-4f3e-bcec-86436703e7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0, 100000, 0.0), {'sessions_over_8h': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes_capped > 8*60)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "Qfvn5Vn7oASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CglM1uhBoASr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88d54a90-60ef-47b9-bcfe-a52876d1d2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((358, 20600, 1.74), {'extreme_age_rows': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "CglM1uhBoASr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kho40L1oASr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94842d29-9239-4894-e1c3-bfcab7a0abde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((24, 22, 2080, 2.21), {'titles_under_15m': 0, 'titles_over_8h': 1, 'total': 2, 'pct_duration_anomaly': 3})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 480) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "2kho40L1oASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yhdFbkaoASr"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "5yhdFbkaoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZK1fkr4oASr"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?"
      ],
      "id": "pZK1fkr4oASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKDRvHtoASr"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "8yKDRvHtoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMy5HEutoASr"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "PMy5HEutoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy3qqUtyoASr"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "Iy3qqUtyoASr"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}